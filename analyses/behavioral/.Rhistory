famavg$norm_fam[5*(i-1)+1:5*i]=c(1:5)
#each level of obj_freq
famavg$mean_resp[5*(i-1)+1]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==1]))
famavg$mean_resp[5*(i-1)+2]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==2]))
famavg$mean_resp[5*(i-1)+3]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==3]))
famavg$mean_resp[5*(i-1)+4]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==4]))
famavg$mean_resp[5*(i-1)+5]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==5]))
#correlation
corr_freq=corr.test(as.numeric(data_freq$Response),data_freq$objective_freq,method="pearson")
freq_frame$SSID[i]=ss_list[i]
freq_frame$task[i]="recent"
freq_frame$pearson_R[i]=corr_freq[1]
corr_fam=corr.test(as.numeric(data_fam$Response),data_fam$norm_fam,method="pearson")
fam_frame$SSID[i]=ss_list[i]
fam_frame$task[i]="lifetime"
fam_frame$pearson_R[i]=corr_fam[1]
}
fam_frame$pearson_R=as.numeric(fam_frame$pearson_R)
freq_frame$pearson_R=as.numeric(freq_frame$pearson_R)
bsize=0.1
library(ggplot2)
#generate ggplots, using ERP data as background
freq.plot=ggplot(data=background_ERP,aes(x=freq)) +
geom_histogram(fill='grey',binwidth = bsize)+
geom_histogram(data=freq_frame,aes(x=pearson_R),binwidth = bsize)+labs(y="participant count", x="frequency correlation")+theme(axis.text=element_text(size=(15)),axis.title=element_text(size=(15)))
fam.plot=ggplot(data=background_ERP,aes(x=fam)) +
geom_histogram(fill='grey',binwidth = bsize)+
geom_histogram(data=fam_frame,aes(x=pearson_R),binwidth=bsize)+labs(y="participant count", x="familiarity correlation")+theme(axis.text=element_text(size=(15)),axis.title=element_text(size=(15)))
View(freqavg)
freqavg$SSID[5*(i-1)+1:5*i]
i
#load data, calculate correlation and mean resp for each level of freq and fam in a for-loop
for (i in c(1:length(ss_list))){
data_dir=paste(datapath,"behavioral\\sub-",ss_list[i],sep="")
data_file=list.files(data_dir,pattern=paste("^",ss_list[i],"_startphase*",sep=""))
data=read_excel(paste(data_dir,"\\",data_file,sep=""))
#remove rows with all NAs but two columns
data=data[rowSums(is.na(data)) != ncol(data)-2, ]
#extract testphase data
data_freq=data[data$task=="recent",]
data_fam=data[data$task=="lifetime",]
#cut the normative fam ratings into 5 levels
norm_fam_qt=quantcut(data_fam$norm_fam,q=5,labels=FALSE)
data_fam$norm_fam_qt=norm_fam_qt
#average
freqavg$SSID[5*(i-1)+1:5*i]=ss_list[i]
freqavg$obj_freq[5*(i-1)+1:5*i]=seq(1,9,2)
#each level of obj_freq
freqavg$mean_resp[5*(i-1)+1]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==1]))
freqavg$mean_resp[5*(i-1)+2]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==3]))
freqavg$mean_resp[5*(i-1)+3]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==5]))
freqavg$mean_resp[5*(i-1)+4]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==7]))
freqavg$mean_resp[5*(i-1)+5]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==9]))
famavg$SSID[5*(i-1)+1:5*i]=ss_list[i]
famavg$norm_fam[5*(i-1)+1:5*i]=c(1:5)
#each level of obj_freq
famavg$mean_resp[5*(i-1)+1]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==1]))
famavg$mean_resp[5*(i-1)+2]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==2]))
famavg$mean_resp[5*(i-1)+3]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==3]))
famavg$mean_resp[5*(i-1)+4]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==4]))
famavg$mean_resp[5*(i-1)+5]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==5]))
#correlation
corr_freq=corr.test(as.numeric(data_freq$Response),data_freq$objective_freq,method="pearson")
freq_frame$SSID[i]=ss_list[i]
freq_frame$task[i]="recent"
freq_frame$pearson_R[i]=corr_freq[1]
corr_fam=corr.test(as.numeric(data_fam$Response),data_fam$norm_fam,method="pearson")
fam_frame$SSID[i]=ss_list[i]
fam_frame$task[i]="lifetime"
fam_frame$pearson_R[i]=corr_fam[1]
}
#correlate frequency judgement with actual presentation frequency, and lifetime fam judgement with normative data.
#using results from my ERP study as background to judge the data quality of the fMRI study.
datapath="C:\\Users\\haozi\\Desktop\\PhD\\fMRI_PrC-PPC_data\\"
library(psych)
library(readxl)
background_ERP=read_excel(paste(datapath,"resource from ERP study\\only_Pearson_R.xlsx",sep=""), sheet = "transposed")
ss_list=c('001','002','003','004','005','006','007','008')
#create empty dataframes to store the correlation values
freq_frame=data.frame(matrix(ncol = 3, nrow = length(ss_list)))
x <- c("pearson_R","SSID","task")
colnames(freq_frame) <- x
fam_frame=freq_frame
#creat empty dataframes to store the mean resp for each level of freq and fam for each ss
freqavg=data.frame(matrix(ncol=3,nrow=length(ss_list)*5))
x=c("mean_resp","SSID","obj_freq")
colnames(freqavg)=x
famavg=data.frame(matrix(ncol=3,nrow=length(ss_list)*5))
x=c("mean_resp","SSID","norm_fam")
colnames(famavg)=x
library(gtools)
#load data, calculate correlation and mean resp for each level of freq and fam in a for-loop
for (i in c(1:length(ss_list))){
data_dir=paste(datapath,"behavioral\\sub-",ss_list[i],sep="")
data_file=list.files(data_dir,pattern=paste("^",ss_list[i],"_startphase*",sep=""))
data=read_excel(paste(data_dir,"\\",data_file,sep=""))
#remove rows with all NAs but two columns
data=data[rowSums(is.na(data)) != ncol(data)-2, ]
#extract testphase data
data_freq=data[data$task=="recent",]
data_fam=data[data$task=="lifetime",]
#cut the normative fam ratings into 5 levels
norm_fam_qt=quantcut(data_fam$norm_fam,q=5,labels=FALSE)
data_fam$norm_fam_qt=norm_fam_qt
#average
freqavg$SSID[5*(i-1)+1:5*i]=ss_list[i]
freqavg$obj_freq[5*(i-1)+1:5*i]=seq(1,9,2)
#each level of obj_freq
freqavg$mean_resp[5*(i-1)+1]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==1]))
freqavg$mean_resp[5*(i-1)+2]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==3]))
freqavg$mean_resp[5*(i-1)+3]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==5]))
freqavg$mean_resp[5*(i-1)+4]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==7]))
freqavg$mean_resp[5*(i-1)+5]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==9]))
famavg$SSID[5*(i-1)+1:5*i]=ss_list[i]
famavg$norm_fam[5*(i-1)+1:5*i]=c(1:5)
#each level of obj_freq
famavg$mean_resp[5*(i-1)+1]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==1]))
famavg$mean_resp[5*(i-1)+2]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==2]))
famavg$mean_resp[5*(i-1)+3]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==3]))
famavg$mean_resp[5*(i-1)+4]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==4]))
famavg$mean_resp[5*(i-1)+5]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==5]))
#correlation
corr_freq=corr.test(as.numeric(data_freq$Response),data_freq$objective_freq,method="pearson")
freq_frame$SSID[i]=ss_list[i]
freq_frame$task[i]="recent"
freq_frame$pearson_R[i]=corr_freq[1]
corr_fam=corr.test(as.numeric(data_fam$Response),data_fam$norm_fam,method="pearson")
fam_frame$SSID[i]=ss_list[i]
fam_frame$task[i]="lifetime"
fam_frame$pearson_R[i]=corr_fam[1]
}
fam_frame$pearson_R=as.numeric(fam_frame$pearson_R)
freq_frame$pearson_R=as.numeric(freq_frame$pearson_R)
bsize=0.1
library(ggplot2)
#generate ggplots, using ERP data as background
freq.plot=ggplot(data=background_ERP,aes(x=freq)) +
geom_histogram(fill='grey',binwidth = bsize)+
geom_histogram(data=freq_frame,aes(x=pearson_R),binwidth = bsize)+labs(y="participant count", x="frequency correlation")+theme(axis.text=element_text(size=(15)),axis.title=element_text(size=(15)))
fam.plot=ggplot(data=background_ERP,aes(x=fam)) +
geom_histogram(fill='grey',binwidth = bsize)+
geom_histogram(data=fam_frame,aes(x=pearson_R),binwidth=bsize)+labs(y="participant count", x="familiarity correlation")+theme(axis.text=element_text(size=(15)),axis.title=element_text(size=(15)))
View(corr_fam)
freq.plot
View(fam_frame)
View(famavg)
5*(i-1)+5
data_dir=paste(datapath,"behavioral\\sub-",ss_list[i],sep="")
data_file=list.files(data_dir,pattern=paste("^",ss_list[i],"_startphase*",sep=""))
data=read_excel(paste(data_dir,"\\",data_file,sep=""))
#remove rows with all NAs but two columns
data=data[rowSums(is.na(data)) != ncol(data)-2, ]
#extract testphase data
data_freq=data[data$task=="recent",]
data_fam=data[data$task=="lifetime",]
#cut the normative fam ratings into 5 levels
norm_fam_qt=quantcut(data_fam$norm_fam,q=5,labels=FALSE)
data_fam$norm_fam_qt=norm_fam_qt
#average
freqavg$SSID[5*(i-1)+1:5*i]=ss_list[i]
freqavg$obj_freq[5*(i-1)+1:5*i]=seq(1,9,2)
#extract testphase data
data_freq=data[data$task=="recent",]
data_fam=data[data$task=="lifetime",]
#cut the normative fam ratings into 5 levels
norm_fam_qt=quantcut(data_fam$norm_fam,q=5,labels=FALSE)
data_fam$norm_fam_qt=norm_fam_qt
#average
freqavg$SSID[5*(i-1)+1:5*i]=ss_list[i]
ss_list[i]
freqavg$SSID[5*(i-1)+1:5*i]=ss_list[i]
class(ss_list[1])
freqavg$SSID[5*(i-1)+1:5*i]
5*(i-1)+1:5*i
(5*(i-1)+1):(5*i)
#correlate frequency judgement with actual presentation frequency, and lifetime fam judgement with normative data.
#using results from my ERP study as background to judge the data quality of the fMRI study.
datapath="C:\\Users\\haozi\\Desktop\\PhD\\fMRI_PrC-PPC_data\\"
library(psych)
library(readxl)
background_ERP=read_excel(paste(datapath,"resource from ERP study\\only_Pearson_R.xlsx",sep=""), sheet = "transposed")
ss_list=c('001','002','003','004','005','006','007','008')
#create empty dataframes to store the correlation values
freq_frame=data.frame(matrix(ncol = 3, nrow = length(ss_list)))
x <- c("pearson_R","SSID","task")
colnames(freq_frame) <- x
fam_frame=freq_frame
#creat empty dataframes to store the mean resp for each level of freq and fam for each ss
freqavg=data.frame(matrix(ncol=3,nrow=length(ss_list)*5))
x=c("mean_resp","SSID","obj_freq")
colnames(freqavg)=x
famavg=data.frame(matrix(ncol=3,nrow=length(ss_list)*5))
x=c("mean_resp","SSID","norm_fam")
colnames(famavg)=x
library(gtools)
#load data, calculate correlation and mean resp for each level of freq and fam in a for-loop
for (i in c(1:length(ss_list))){
data_dir=paste(datapath,"behavioral\\sub-",ss_list[i],sep="")
data_file=list.files(data_dir,pattern=paste("^",ss_list[i],"_startphase*",sep=""))
data=read_excel(paste(data_dir,"\\",data_file,sep=""))
#remove rows with all NAs but two columns
data=data[rowSums(is.na(data)) != ncol(data)-2, ]
#extract testphase data
data_freq=data[data$task=="recent",]
data_fam=data[data$task=="lifetime",]
#cut the normative fam ratings into 5 levels
norm_fam_qt=quantcut(data_fam$norm_fam,q=5,labels=FALSE)
data_fam$norm_fam_qt=norm_fam_qt
#average
freqavg$SSID[(5*(i-1)+1):(5*i)]=ss_list[i]
freqavg$obj_freq[(5*(i-1)+1):(5*i)]=seq(1,9,2)
#each level of obj_freq
freqavg$mean_resp[5*(i-1)+1]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==1]))
freqavg$mean_resp[5*(i-1)+2]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==3]))
freqavg$mean_resp[5*(i-1)+3]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==5]))
freqavg$mean_resp[5*(i-1)+4]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==7]))
freqavg$mean_resp[5*(i-1)+5]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==9]))
famavg$SSID[(5*(i-1)+1):(5*i)]=ss_list[i]
famavg$norm_fam[(5*(i-1)+1):(5*i)]=c(1:5)
#each level of obj_freq
famavg$mean_resp[5*(i-1)+1]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==1]))
famavg$mean_resp[5*(i-1)+2]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==2]))
famavg$mean_resp[5*(i-1)+3]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==3]))
famavg$mean_resp[5*(i-1)+4]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==4]))
famavg$mean_resp[5*(i-1)+5]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==5]))
#correlation
corr_freq=corr.test(as.numeric(data_freq$Response),data_freq$objective_freq,method="pearson")
freq_frame$SSID[i]=ss_list[i]
freq_frame$task[i]="recent"
freq_frame$pearson_R[i]=corr_freq[1]
corr_fam=corr.test(as.numeric(data_fam$Response),data_fam$norm_fam,method="pearson")
fam_frame$SSID[i]=ss_list[i]
fam_frame$task[i]="lifetime"
fam_frame$pearson_R[i]=corr_fam[1]
}
fam_frame$pearson_R=as.numeric(fam_frame$pearson_R)
freq_frame$pearson_R=as.numeric(freq_frame$pearson_R)
bsize=0.1
library(ggplot2)
#generate ggplots, using ERP data as background
freq.plot=ggplot(data=background_ERP,aes(x=freq)) +
geom_histogram(fill='grey',binwidth = bsize)+
geom_histogram(data=freq_frame,aes(x=pearson_R),binwidth = bsize)+labs(y="participant count", x="frequency correlation")+theme(axis.text=element_text(size=(15)),axis.title=element_text(size=(15)))
fam.plot=ggplot(data=background_ERP,aes(x=fam)) +
geom_histogram(fill='grey',binwidth = bsize)+
geom_histogram(data=fam_frame,aes(x=pearson_R),binwidth=bsize)+labs(y="participant count", x="familiarity correlation")+theme(axis.text=element_text(size=(15)),axis.title=element_text(size=(15)))
#generate barplots as in Devin's paper
View(famavg)
View(famavg)
View(freqavg)
i=2
data_dir=paste(datapath,"behavioral\\sub-",ss_list[i],sep="")
data_file=list.files(data_dir,pattern=paste("^",ss_list[i],"_startphase*",sep=""))
data=read_excel(paste(data_dir,"\\",data_file,sep=""))
#remove rows with all NAs but two columns
data=data[rowSums(is.na(data)) != ncol(data)-2, ]
#extract testphase data
data_freq=data[data$task=="recent",]
data_fam=data[data$task=="lifetime",]
#cut the normative fam ratings into 5 levels
norm_fam_qt=quantcut(data_fam$norm_fam,q=5,labels=FALSE)
data_fam$norm_fam_qt=norm_fam_qt
View(data_fam)
mean(as.numeric(data_freq$Response[data_freq$objective_freq==1]
)
)
?mean
#correlate frequency judgement with actual presentation frequency, and lifetime fam judgement with normative data.
#using results from my ERP study as background to judge the data quality of the fMRI study.
datapath="C:\\Users\\haozi\\Desktop\\PhD\\fMRI_PrC-PPC_data\\"
library(psych)
library(readxl)
background_ERP=read_excel(paste(datapath,"resource from ERP study\\only_Pearson_R.xlsx",sep=""), sheet = "transposed")
ss_list=c('001','002','003','004','005','006','007','008')
#create empty dataframes to store the correlation values
freq_frame=data.frame(matrix(ncol = 3, nrow = length(ss_list)))
x <- c("pearson_R","SSID","task")
colnames(freq_frame) <- x
fam_frame=freq_frame
#creat empty dataframes to store the mean resp for each level of freq and fam for each ss
freqavg=data.frame(matrix(ncol=3,nrow=length(ss_list)*5))
x=c("mean_resp","SSID","obj_freq")
colnames(freqavg)=x
famavg=data.frame(matrix(ncol=3,nrow=length(ss_list)*5))
x=c("mean_resp","SSID","norm_fam")
colnames(famavg)=x
library(gtools)
#load data, calculate correlation and mean resp for each level of freq and fam in a for-loop
for (i in c(1:length(ss_list))){
data_dir=paste(datapath,"behavioral\\sub-",ss_list[i],sep="")
data_file=list.files(data_dir,pattern=paste("^",ss_list[i],"_startphase*",sep=""))
data=read_excel(paste(data_dir,"\\",data_file,sep=""))
#remove rows with all NAs but two columns
data=data[rowSums(is.na(data)) != ncol(data)-2, ]
#extract testphase data
data_freq=data[data$task=="recent",]
data_fam=data[data$task=="lifetime",]
#cut the normative fam ratings into 5 levels
norm_fam_qt=quantcut(data_fam$norm_fam,q=5,labels=FALSE)
data_fam$norm_fam_qt=norm_fam_qt
#average
freqavg$SSID[(5*(i-1)+1):(5*i)]=ss_list[i]
freqavg$obj_freq[(5*(i-1)+1):(5*i)]=seq(1,9,2)
#each level of obj_freq
freqavg$mean_resp[5*(i-1)+1]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==1]),na.rm=TRUE)
freqavg$mean_resp[5*(i-1)+2]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==3]),na.rm=TRUE)
freqavg$mean_resp[5*(i-1)+3]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==5]),na.rm=TRUE)
freqavg$mean_resp[5*(i-1)+4]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==7]),na.rm=TRUE)
freqavg$mean_resp[5*(i-1)+5]=mean(as.numeric(data_freq$Response[data_freq$objective_freq==9]),na.rm=TRUE)
famavg$SSID[(5*(i-1)+1):(5*i)]=ss_list[i]
famavg$norm_fam[(5*(i-1)+1):(5*i)]=c(1:5)
#each level of obj_freq
famavg$mean_resp[5*(i-1)+1]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==1]),na.rm=TRUE)
famavg$mean_resp[5*(i-1)+2]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==2]),na.rm=TRUE)
famavg$mean_resp[5*(i-1)+3]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==3]),na.rm=TRUE)
famavg$mean_resp[5*(i-1)+4]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==4]),na.rm=TRUE)
famavg$mean_resp[5*(i-1)+5]=mean(as.numeric(data_fam$Response[data_fam$norm_fam_qt==5]),na.rm=TRUE)
#correlation
corr_freq=corr.test(as.numeric(data_freq$Response),data_freq$objective_freq,method="pearson")
freq_frame$SSID[i]=ss_list[i]
freq_frame$task[i]="recent"
freq_frame$pearson_R[i]=corr_freq[1]
corr_fam=corr.test(as.numeric(data_fam$Response),data_fam$norm_fam,method="pearson")
fam_frame$SSID[i]=ss_list[i]
fam_frame$task[i]="lifetime"
fam_frame$pearson_R[i]=corr_fam[1]
}
fam_frame$pearson_R=as.numeric(fam_frame$pearson_R)
freq_frame$pearson_R=as.numeric(freq_frame$pearson_R)
bsize=0.1
library(ggplot2)
#generate ggplots, using ERP data as background
freq.plot=ggplot(data=background_ERP,aes(x=freq)) +
geom_histogram(fill='grey',binwidth = bsize)+
geom_histogram(data=freq_frame,aes(x=pearson_R),binwidth = bsize)+labs(y="participant count", x="frequency correlation")+theme(axis.text=element_text(size=(15)),axis.title=element_text(size=(15)))
fam.plot=ggplot(data=background_ERP,aes(x=fam)) +
geom_histogram(fill='grey',binwidth = bsize)+
geom_histogram(data=fam_frame,aes(x=pearson_R),binwidth=bsize)+labs(y="participant count", x="familiarity correlation")+theme(axis.text=element_text(size=(15)),axis.title=element_text(size=(15)))
#generate barplots as in Devin's paper
View(famavg)
View(freqavg)
freq.bar=ggplot(freqavg,aes(x=obj_freq,y=mean_resp))+geom_col()
freq.bar
install.packages("dplyr")
library(dplyr)
#use piping to chain functions
freq.sum=freqavg %>%
group_by(obj_freq) %>%
summarise(sub_mean=mean(mean_resp),sub_sd=sd(mean_resp),sub_se=sd(mean_resp)/sqrt(n()))
View(freq.sum)
freq.bar=ggplot(freq.sum,aes(x=obj_freq,y=sub_mean))+geom_col()+geom_errorbar(aes(ymin=sub_mean-sub_se,ymax=sub_mean+sub_se),width=0.2)
freq.bar
fam.sum=famavg %>%
group_by(obj_fam) %>%
summarise(sub_mean=mean(mean_resp),sub_sd=sd(mean_resp),sub_se=sd(mean_resp)/sqrt(n()))
fam.bar=ggplot(fam.sum,aes(x=obj_fam,y=sub_mean))+geom_col()+geom_errorbar(aes(ymin=sub_mean-sub_se,ymax=sub_mean+sub_se),width=0.2)
fam.sum=famavg %>%
group_by(norm_fam) %>%
summarise(sub_mean=mean(mean_resp),sub_sd=sd(mean_resp),sub_se=sd(mean_resp)/sqrt(n()))
fam.bar=ggplot(fam.sum,aes(x=obj_fam,y=sub_mean))+geom_col()+geom_errorbar(aes(ymin=sub_mean-sub_se,ymax=sub_mean+sub_se),width=0.2)
fam.bar
fam.sum=famavg %>%
group_by(norm_fam) %>%
summarise(sub_mean=mean(mean_resp),sub_sd=sd(mean_resp),sub_se=sd(mean_resp)/sqrt(n()))
fam.bar=ggplot(fam.sum,aes(x=norm_fam,y=sub_mean))+geom_col()+geom_errorbar(aes(ymin=sub_mean-sub_se,ymax=sub_mean+sub_se),width=0.2)
fam.bar
freq.bar
library(rio)
library(plyr)
SSID=c('001','002','003','004','005','006','007','008','010','011','012','013')
i=1
temp_data=import(paste0("C:\\Users\\haozi\\Desktop\\PhD\\fMRI_PrC-PPC_data\\behavioral\\sub-",SSID[i],'\\',SSID[i],'_startphase-study_startrun-1_starttrial-1_data.xlsx'))
animacy_data=temp_data[temp_data$task=='animacy',]
ps_data=temp_data[temp_data$task=='post_scan',]
words.9=unique(animacy_data$Stimuli[animacy_data$objective_freq==91])
words.9=words.9[!is.na(words.9)]
words.1=unique(animacy_data$Stimuli[animacy_data$objective_freq==11])
words.1=words.1[!is.na(words.1)]
norm.9=animacy_data$norm_fam[animacy_data$objective_freq==91&animacy_data$Stimuli %in% words.9]
norm.1=animacy_data$norm_fam[animacy_data$objective_freq==11&animacy_data$Stimuli %in% words.1]
post.9=ps_data$Response[ps_data$Stimuli %in% words.9]
post.1=ps_data$Response[ps_data$Stimuli %in% words.1]
(post.9-1)/8
post.9-1
? rep
post.9-c(rep(1,length(post.9)))
class(post.9)
norm.9-1
as.numeric(post.9)
post.9=as.numeric(ps_data$Response[ps_data$Stimuli %in% words.9])
post.1=as.numeric(ps_data$Response[ps_data$Stimuli %in% words.1])
norm.9.st=(norm.9-1)/8
norm.1.st=(norm.1-1)/8
post.9.st=(post.9-1)/5
post.1.st=(post.1-1)/5
#calculate the avg. post-pre differences
post_pre.9=post.9.st-norm.9.st
post_pre.1=post.1.st-norm.1.st
pp.9.mean=mean(post_pre.9)
pp.1.mean=mean(post_pre.1)
data=data.frame()
colnames(data)=c('pp_9','pp_1')
#for each subject, calculate the average difference of normative lifetime ratings between the two most extreme bins, and the average difference in post-scan ratings
data=data.frame(matrix(1,2))
View(data)
#for each subject, calculate the average difference of normative lifetime ratings between the two most extreme bins, and the average difference in post-scan ratings
data=data.frame(matrix(2,1))
#for each subject, calculate the average difference of normative lifetime ratings between the two most extreme bins, and the average difference in post-scan ratings
data=data.frame(pp_9=as.numeric(),pp_1=as.numeric())
View(data)
#for each subject, calculate the average difference of normative lifetime ratings between the two most extreme bins, and the average difference in post-scan ratings
data=data.frame(pp_9=as.numeric(),pp_1=as.numeric(),SSID=as.character())
#check if repetition in the study phase boosted lifetime familiarity ratings post-scan
#use normtive lifetime ratings as baseline, compare post-pre (subject rating - normative rating) lifetime rating difference for words presented 9 times with those presented 1 times.
#This is more sensitive than just correlating lifetime rating with presentation frequency since it takes the baseline difference of lifetime ratings between the bins into consideration. Also, it is likely that the repetition only had a small effect on lifetime ratings, which may not give a significant correlation.
library(rio)
library(plyr)
SSID=c('001','002','003','004','005','006','007','008','010','011','012','013')
#for each subject, calculate the average difference of normative lifetime ratings between the two most extreme bins, and the average difference in post-scan ratings
data=data.frame(pp_9=as.numeric(),pp_1=as.numeric(),SSID=as.character())
for (i in c(1:length(SSID))){
temp_data=import(paste0("C:\\Users\\haozi\\Desktop\\PhD\\fMRI_PrC-PPC_data\\behavioral\\sub-",SSID[i],'\\',SSID[i],'_startphase-study_startrun-1_starttrial-1_data.xlsx'))
animacy_data=temp_data[temp_data$task=='animacy',]
ps_data=temp_data[temp_data$task=='post_scan',]
words.9=unique(animacy_data$Stimuli[animacy_data$objective_freq==91])
words.9=words.9[!is.na(words.9)]
words.1=unique(animacy_data$Stimuli[animacy_data$objective_freq==11])
words.1=words.1[!is.na(words.1)]
norm.9=animacy_data$norm_fam[animacy_data$objective_freq==91&animacy_data$Stimuli %in% words.9]
norm.1=animacy_data$norm_fam[animacy_data$objective_freq==11&animacy_data$Stimuli %in% words.1]
post.9=as.numeric(ps_data$Response[ps_data$Stimuli %in% words.9])
post.1=as.numeric(ps_data$Response[ps_data$Stimuli %in% words.1])
#range normalization since norm and post-scan ratings are on different scales
norm.9.st=(norm.9-1)/8
norm.1.st=(norm.1-1)/8
post.9.st=(post.9-1)/5
post.1.st=(post.1-1)/5
#calculate the avg. post-pre differences
post_pre.9=post.9.st-norm.9.st
post_pre.1=post.1.st-norm.1.st
pp.9.mean=mean(post_pre.9)
pp.1.mean=mean(post_pre.1)
data=rbind(data,data.frame(pp.9.mean,pp.1.mean,SSID[i]))
}
#t-test compare pre-post lifetime rating differences
View(data)
i=10
temp_data=import(paste0("C:\\Users\\haozi\\Desktop\\PhD\\fMRI_PrC-PPC_data\\behavioral\\sub-",SSID[i],'\\',SSID[i],'_startphase-study_startrun-1_starttrial-1_data.xlsx'))
View(temp_data)
animacy_data=temp_data[temp_data$task=='animacy',]
ps_data=temp_data[temp_data$task=='post_scan',]
words.9=unique(animacy_data$Stimuli[animacy_data$objective_freq==91])
words.9=words.9[!is.na(words.9)]
words.1=unique(animacy_data$Stimuli[animacy_data$objective_freq==11])
words.1=words.1[!is.na(words.1)]
norm.9=animacy_data$norm_fam[animacy_data$objective_freq==91&animacy_data$Stimuli %in% words.9]
norm.1=animacy_data$norm_fam[animacy_data$objective_freq==11&animacy_data$Stimuli %in% words.1]
post.9=as.numeric(ps_data$Response[ps_data$Stimuli %in% words.9])
post.1=as.numeric(ps_data$Response[ps_data$Stimuli %in% words.1])
#range normalization since norm and post-scan ratings are on different scales
norm.9.st=(norm.9-1)/8
norm.1.st=(norm.1-1)/8
post.9.st=(post.9-1)/5
post.1.st=(post.1-1)/5
#calculate the avg. post-pre differences
post_pre.9=post.9.st-norm.9.st
post_pre.1=post.1.st-norm.1.st
pp.9.mean=mean(post_pre.9)
pp.1.mean=mean(post_pre.1)
norm.9
norm.1
post.1
post.9
post_pre.9
post_pre.1
#check if repetition in the study phase boosted lifetime familiarity ratings post-scan
#use normtive lifetime ratings as baseline, compare post-pre (subject rating - normative rating) lifetime rating difference for words presented 9 times with those presented 1 times.
#This is more sensitive than just correlating lifetime rating with presentation frequency since it takes the baseline difference of lifetime ratings between the bins into consideration. Also, it is likely that the repetition only had a small effect on lifetime ratings, which may not give a significant correlation.
library(rio)
library(plyr)
SSID=c('001','002','003','004','005','006','007','008','010','011','012','013')
#for each subject, calculate the average difference of normative lifetime ratings between the two most extreme bins, and the average difference in post-scan ratings
data=data.frame(pp_9=as.numeric(),pp_1=as.numeric(),SSID=as.character())
for (i in c(1:length(SSID))){
temp_data=import(paste0("C:\\Users\\haozi\\Desktop\\PhD\\fMRI_PrC-PPC_data\\behavioral\\sub-",SSID[i],'\\',SSID[i],'_startphase-study_startrun-1_starttrial-1_data.xlsx'))
animacy_data=temp_data[temp_data$task=='animacy',]
ps_data=temp_data[temp_data$task=='post_scan',]
words.9=unique(animacy_data$Stimuli[animacy_data$objective_freq==91])
words.9=words.9[!is.na(words.9)]
words.1=unique(animacy_data$Stimuli[animacy_data$objective_freq==11])
words.1=words.1[!is.na(words.1)]
norm.9=animacy_data$norm_fam[animacy_data$objective_freq==91&animacy_data$Stimuli %in% words.9]
norm.1=animacy_data$norm_fam[animacy_data$objective_freq==11&animacy_data$Stimuli %in% words.1]
post.9=as.numeric(ps_data$Response[ps_data$Stimuli %in% words.9])
post.1=as.numeric(ps_data$Response[ps_data$Stimuli %in% words.1])
#range normalization since norm and post-scan ratings are on different scales
norm.9.st=(norm.9-1)/8
norm.1.st=(norm.1-1)/8
post.9.st=(post.9-1)/5
post.1.st=(post.1-1)/5
#calculate the avg. post-pre differences
post_pre.9=post.9.st-norm.9.st
post_pre.1=post.1.st-norm.1.st
pp.9.mean=mean(post_pre.9,na.rm=TRUE)
pp.1.mean=mean(post_pre.1,na.rm=TRUE)
data=rbind(data,data.frame(pp.9.mean,pp.1.mean,SSID[i]))
}
#t-test compare pre-post lifetime rating differences
View(data)
#t-test compare pre-post lifetime rating differences
pp.9_1.t=t.test(data$pp.9.mean,data$pp.1.mean,paired = TRUE)
pp.9_1.t
